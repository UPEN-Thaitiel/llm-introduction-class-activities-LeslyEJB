{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80a7744d",
   "metadata": {},
   "source": [
    "### Accesing to Ollama API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373d261d",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13c50ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import dotenv\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f766684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llamas a ollama en cmd con ollama run llama3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b052cfbe",
   "metadata": {},
   "source": [
    "### Setting Up API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b691eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"llama3.2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af243bee",
   "metadata": {},
   "source": [
    "### Generating Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "590dfe97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _chat_ollama_sdk(messages, temperature=None, max_tokens=None):\n",
    "    opts = {}\n",
    "    if temperature is not None:\n",
    "        opts[\"temperature\"] = float(temperature)\n",
    "    if max_tokens is not None:\n",
    "        opts[\"num_predict\"] = int(max_tokens)\n",
    "\n",
    "    resp = ollama.chat(\n",
    "        model=MODEL,\n",
    "        messages=messages,\n",
    "        options=opts or None\n",
    "    )\n",
    "    return resp[\"message\"][\"content\"].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "430a9b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_llama32(prompt, max_tokens=50, temperature=0.7):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a concise and precise assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    return _chat_ollama_sdk(messages, temperature=temperature, max_tokens=max_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fdd75471",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"How are you today?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8436fa8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm functioning properly, ready to assist with your queries. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "print(ask_llama32(\"How are you today?\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8e8bf8",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a19f35a",
   "metadata": {},
   "source": [
    "### Summarising Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e86a4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_summarizer_llama32(prompt, temperature=0.5, max_tokens=256):\n",
    "    messages = [\n",
    "        {\"role\": \"system\",\n",
    "         \"content\": \"Extract a comma-separated list of keywords from the provided text.\"},\n",
    "        {\"role\": \"user\",\n",
    "         \"content\": \"A flying saucer ... (same example as above)\"},\n",
    "        {\"role\": \"assistant\",\n",
    "         \"content\": \"flying saucer, guest house, 7ft alien-like figure, hedge, cigar-shaped UFO, school yard, extraterrestrial encounters, UK, mass sightings, Broad Haven, Bermuda Triangle, strange beings, late seventies, Netflix documentary, Steven Spielberg, 1977, Cold War, Star Wars, Close Encounters of the Third Kind\"},\n",
    "        {\"role\": \"user\",\n",
    "         \"content\": \"Each April ... (same second example)\"},\n",
    "        {\"role\": \"assistant\",\n",
    "         \"content\": \"April, Maeliya, northwest Sri Lanka, banyan tree, wewa, reservoir, tank, Sinhala, rice paddies, 175-acres, rainwater, agrarian committee, coconut milk, blessings, prosperous harvest, deities, sluice gate, irrigation canals, dry months, lake-like water bodies, farmers, Sinhala phrase, village life, pagoda, temple\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    return _chat_ollama_sdk(messages, temperature=temperature, max_tokens=max_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "76b99101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Master Reef Guide Kirsty Whitman didn't need to tell me twice. Peering down through my snorkel mask in the direction of her pointed finger, I spotted a huge male manta ray trailing a female in perfect sync – an effort to impress a potential mate, exactly as Whitman had described during her animated presentation the previous evening. Having some knowledge of what was unfolding before my eyes on our snorkelling safari made the encounter even more magical as I kicked against the current to admire this intimate undersea ballet for a few precious seconds more.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Master Reef Guide Kirsty Whitman didn't need to tell me twice. Peering down through my snorkel mask in the direction of her pointed finger, I spotted a huge male manta ray trailing a female in perfect sync – an effort to impress a potential mate, exactly as Whitman had described during her animated presentation the previous evening. Having some knowledge of what was unfolding before my eyes on our snorkelling safari made the encounter even more magical as I kicked against the current to admire this intimate undersea ballet for a few precious seconds more.\"\n",
    "print(prompt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "22a559d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The origin of cheese is not precisely known, but it is believed to have been discovered accidentally by nomadic tribes in the Middle East around 8000'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_summarizer_llama32(prompt, max_tokens=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd39fb3d",
   "metadata": {},
   "source": [
    "### Coding a simple chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2608edf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def poetic_chatbot_llama32(prompt, temperature=1.0, max_tokens=256):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a poetic chatbot. Answer in short rhymed couplets.\"},\n",
    "        {\"role\": \"user\", \"content\": \"When was Google founded?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"In nineteen ninety-eight the vision came to be,\\nBy Page and Brin, a search light for all to see.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Which country has the youngest president?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Austria crowned its youth with Kurz’s earnest run,\\nAt thirty-one he led, beneath a cautious sun.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    return _chat_ollama_sdk(messages, temperature=temperature, max_tokens=max_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "16d43869",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ancient nomads in Turkey first tried to obtain,\\nBy drying curds, and binding them with thread and bone.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"When was cheese first made?\"\n",
    "poetic_chatbot_llama32(prompt, temperature=1.0, max_tokens=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606e7e68",
   "metadata": {},
   "source": [
    "#### Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9d3d23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
